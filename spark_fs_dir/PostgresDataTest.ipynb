{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff1e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a5631c3b-ecf6-4db8-90f6-9b81823614fb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.5.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 103ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a5631c3b-ecf6-4db8-90f6-9b81823614fb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 14:59:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import py4j\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-test-postgres-results\").\\\n",
    "        master(\"spark://spark:7077\").\\\n",
    "        config('spark.jars.packages', 'org.postgresql:postgresql:42.5.1').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb487a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_connection_string = \"jdbc:postgresql://test-postgres-db-%d:5432/db\"\n",
    "dbtable = \"tbl\"\n",
    "username = \"auser\"\n",
    "passwd = \"1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPostgresDataInSync:\n",
    "    def __init__(self, spark_session, gen_connection_string, dbtable, username, passwd):\n",
    "            self.df1 = self.load(spark_session, gen_connection_string%(1), dbtable, username, passwd).cache()\n",
    "            self.df2 = self.load(spark_session, gen_connection_string%(2), dbtable, username, passwd).cache()\n",
    "            \n",
    "            \n",
    "    def load(self, spark_session, connection_string, dbtable, username, passwd):\n",
    "        return spark_session \\\n",
    "                        .read \\\n",
    "                        .format(\"jdbc\") \\\n",
    "                        .option(\"url\", connection_string) \\\n",
    "                        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                        .option(\"dbtable\", dbtable) \\\n",
    "                        .option(\"user\", username) \\\n",
    "                        .option(\"password\", passwd) \\\n",
    "                        .load()\n",
    "    \n",
    "    def count(self):\n",
    "        cnt1 = self.__count(self.df1)\n",
    "        cnt2 = self.__count(self.df2)\n",
    "        print(\"Data available in test-postgres-db-1 : %d\"%(cnt1))\n",
    "        print(\"Data available in test-postgres-db-2 : %d\"%(cnt2))\n",
    "    \n",
    "    def __count(self, df):\n",
    "        return df.count()\n",
    "    \n",
    "    def count_distinct(self):\n",
    "        cnt1 = self.__count_distinct(self.df1)\n",
    "        cnt2 = self.__count_distinct(self.df2)\n",
    "        print(\"Distinct data available in test-postgres-db-1 : %d\"%(cnt1))\n",
    "        print(\"Distinct data available in test-postgres-db-1 : %d\"%(cnt2))\n",
    "        assert cnt1 == cnt2\n",
    "    \n",
    "    def __count_distinct(self, df):\n",
    "        return df.select(col(\"val1\"), col(\"val2\")).distinct().count()\n",
    "    \n",
    "\n",
    "    def assert_dataframe_equality(self):\n",
    "        assert self.__count_distinct(self.df1) == self.__count_distinct(self.df2) == \\\n",
    "                    self.__count_distinct( \n",
    "                                self.df1.join(self.df2, \\\n",
    "                                            [\"val1\", \"val2\"], \\\n",
    "                                              \"inner\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d781f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = TestPostgresDataInSync(spark, gen_connection_string, dbtable, username, passwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66bebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data available in test-postgres-db-1 : 524\n",
      "Data available in test-postgres-db-2 : 262\n"
     ]
    }
   ],
   "source": [
    "tester.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49be0420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct data available in test-postgres-db-1 : 259\n",
      "Distinct data available in test-postgres-db-1 : 259\n"
     ]
    }
   ],
   "source": [
    "tester.count_distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3221ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tester.assert_dataframe_equality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da650e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
